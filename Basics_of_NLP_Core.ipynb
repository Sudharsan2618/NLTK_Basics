{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP — Core (Beginner Friendly)\n",
    "\n",
    "This notebook teaches the core basics of NLP only. No chatbot, no translator — just fundamental concepts with small, clear examples.\n",
    "\n",
    "We'll cover:\n",
    "- Lexical Analysis (sentences, words, stopwords, stemming, lemmatization)\n",
    "- Syntactic Analysis (POS tagging, simple chunking)\n",
    "- Semantic Analysis (WordNet, simple similarity, basic NER)\n",
    "- Discourse Analysis (links across sentences)\n",
    "- Pragmatic Analysis (basic intent via simple rules)\n",
    "\n",
    "Run cells from top to bottom. If anything fails, re-run the Setup cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Prerequisites and Installation (Step-by-step)\n",
    "You said you have Python and pip. Great! If you're running this locally, do the following once.\n",
    "\n",
    "\n",
    "\n",
    "- Install required package:\n",
    "```powershell\n",
    "pip install nltk\n",
    "```\n",
    "You can also run the next cell to install from inside the notebook (uncomment the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK from inside the notebook if needed (uncomment):\n",
    "# !pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download essential NLTK data (safe to run multiple times)\n",
    "packages = [\n",
    "    'punkt',                # tokenizers\n",
    "    'stopwords',            # stopwords list\n",
    "    'wordnet',              # WordNet for lemmatization/semantics\n",
    "    'omw-1.4',              # WordNet multilingual data\n",
    "    'averaged_perceptron_tagger',  # POS tagger\n",
    "    'maxent_ne_chunker',    # NER chunker\n",
    "    'words'                 # word list for NER\n",
    "]\n",
    "for p in packages:\n",
    "    try:\n",
    "        nltk.download(p, quiet=False)\n",
    "    except Exception as e:\n",
    "        print(f'Could not download {p}:', e)\n",
    "\n",
    "print('Setup complete. ✅ If downloads failed, check your internet and rerun this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lexical Analysis — Tokenization and Normalization\n",
    "Lexical analysis breaks text into sentences/words and normalizes it.\n",
    "We'll do: sentence tokenization, word tokenization, stopword removal, stemming, lemmatization, regex tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "text = (\n",
    "    \"NLP is fun! It helps computers understand language.\n",
    "    Machine learning and linguistics are powerful together.\"\n",
    ")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print('Sentences:', sentences)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print('Words:', words)\n",
    "\n",
    "# Lowercase + remove stopwords/punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_clean = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "print('Clean words:', words_clean)\n",
    "\n",
    "# Stemming vs Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stems = [stemmer.stem(w) for w in words_clean]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words_clean]\n",
    "print('Stems:', stems)\n",
    "print('Lemmas:', lemmas)\n",
    "\n",
    "# Regex tokenizer: words with 2+ letters\n",
    "regex_tok = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
    "print('Regex tokens:', regex_tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Syntactic Analysis — POS Tagging and Chunking\n",
    "Syntactic analysis finds grammatical structure. We'll tag parts-of-speech and extract simple noun phrases (NP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = word_tokenize(\"John bought a new laptop from the store in Chennai.\")\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print('POS tags:', pos_tags)\n",
    "\n",
    "# Simple NP chunk grammar: optional determiner, any adjectives, then a noun\n",
    "grammar = r\"NP: {<DT>?<JJ>*<NN.*>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(pos_tags)\n",
    "print(tree)  # text-based parse tree\n",
    "# Optional GUI view if supported locally:\n",
    "# tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Semantic Analysis — Word Meaning and Entities\n",
    "We'll use WordNet for synonyms/definitions, a simple similarity measure, and NLTK's basic NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'computer'\n",
    "synsets = wn.synsets(word)\n",
    "print(f'Synsets for {word!r}:')\n",
    "for s in synsets[:3]:\n",
    "    print('-', s.name(), '| definition:', s.definition())\n",
    "\n",
    "# Simple path similarity between two senses\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print('Similarity(dog, cat):', dog.path_similarity(cat))\n",
    "\n",
    "# Named Entity Recognition (basic)\n",
    "sentence = \"Google hired Sundar Pichai in California.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "ner_tree = nltk.ne_chunk(tags)\n",
    "print(ner_tree)\n",
    "# ner_tree.draw()  # optional GUI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Discourse Analysis — Across Sentences\n",
    "We look for simple discourse markers and repeated content words to see how sentences connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "paragraph = (\n",
    "    \"Ravi bought a phone. He liked the camera; however, the battery was weak.\n",
    "    Therefore, he returned the phone.\"\n",
    ")\n",
    "sents = sent_tokenize(paragraph)\n",
    "markers = {'however', 'therefore', 'moreover', 'meanwhile', 'furthermore', 'nevertheless'}\n",
    "\n",
    "print('Sentences:')\n",
    "for i, s in enumerate(sents, 1):\n",
    "    found = [m for m in markers if m in s.lower()]\n",
    "    print(f'{i}.', s, ('| markers: ' + ', '.join(found)) if found else '')\n",
    "\n",
    "# Very naive repetition tracker\n",
    "all_words = [w.lower() for w in word_tokenize(paragraph) if w.isalpha()]\n",
    "counts = Counter(all_words)\n",
    "print('Repeated content words:', [w for w, c in counts.items() if c > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Pragmatic Analysis — Intent (Very Simple)\n",
    "Pragmatics looks at meaning in context. We'll create a tiny rule-based intent guesser to show the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_intent(utterance: str) -> str:\n",
    "    u = utterance.lower().strip()\n",
    "    if any(x in u for x in ['price', 'cost', 'how much']):\n",
    "        return 'intent: ask_price'\n",
    "    if any(x in u for x in ['hi', 'hello', 'hey']):\n",
    "        return 'intent: greeting'\n",
    "    if any(x in u for x in ['bye', 'goodbye', 'see you']):\n",
    "        return 'intent: farewell'\n",
    "    return 'intent: unknown'\n",
    "\n",
    "for s in ['Hello!', 'How much is this?', 'Ok bye', 'Can you help?']:\n",
    "    print(s, '->', guess_intent(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercises\n",
    "- Change the input text and observe tokenization differences.\n",
    "- Add your own stopwords (domain-specific words) and re-run.\n",
    "- Try LancasterStemmer or SnowballStemmer and compare to PorterStemmer.\n",
    "- Modify the chunk grammar (e.g., capture prepositional phrases).\n",
    "- Look up different WordNet synsets and compare similarities.\n",
    "- Extend `guess_intent()` with more rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
