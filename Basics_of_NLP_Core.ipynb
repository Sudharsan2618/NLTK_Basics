{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP — Core (Beginner Friendly)\n",
    "\n",
    "This notebook teaches the core basics of NLP only. No chatbot, no translator — just fundamental concepts with small, clear examples.\n",
    "\n",
    "We'll cover:\n",
    "- Lexical Analysis (sentences, words, stopwords, stemming, lemmatization)\n",
    "- Syntactic Analysis (POS tagging, simple chunking)\n",
    "- Semantic Analysis (WordNet, simple similarity, basic NER)\n",
    "- Discourse Analysis (links across sentences)\n",
    "- Pragmatic Analysis (basic intent via simple rules)\n",
    "\n",
    "Run cells from top to bottom. If anything fails, re-run the Setup cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Prerequisites and Installation (Step-by-step)\n",
    "You said you have Python and pip. Great! If you're running this locally, do the following once.\n",
    "\n",
    "\n",
    "\n",
    "- Install required package:\n",
    "```powershell\n",
    "pip install nltk\n",
    "```\n",
    "You can also run the next cell to install from inside the notebook (uncomment the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\WELCOME\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. ✅ If downloads failed, check your internet and rerun this cell.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK from inside the notebook if needed (uncomment):\n",
    "# !pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download essential NLTK data (safe to run multiple times)\n",
    "packages = [\n",
    "    'punkt',\n",
    "    'punkt_tab',               # tokenizers\n",
    "    'stopwords',            # stopwords list\n",
    "    'wordnet',              # WordNet for lemmatization/semantics\n",
    "    'omw-1.4',              # WordNet multilingual data\n",
    "    'averaged_perceptron_tagger',  # POS tagger\n",
    "    'averaged_perceptron_tagger_eng',  # new name\n",
    "    'maxent_ne_chunker',    # NER chunker\n",
    "    'maxent_ne_chunker_tab',\n",
    "    'words'                 # word list for NER\n",
    "]\n",
    "for p in packages:\n",
    "    try:\n",
    "        nltk.download(p, quiet=False)\n",
    "    except Exception as e:\n",
    "        print(f'Could not download {p}:', e)\n",
    "\n",
    "print('Setup complete. ✅ If downloads failed, check your internet and rerun this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lexical Analysis — Tokenization and Normalization\n",
    "Lexical analysis breaks text into sentences/words and normalizes it.\n",
    "We'll do: sentence tokenization, word tokenization, stopword removal, stemming, lemmatization, regex tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['NLP is fun!', 'It helps computers understand language.', 'Machine learning and linguistics are powerful together.']\n",
      "Words: ['NLP', 'is', 'fun', '!', 'It', 'helps', 'computers', 'understand', 'language', '.', 'Machine', 'learning', 'and', 'linguistics', 'are', 'powerful', 'together', '.']\n",
      "Clean words: ['nlp', 'fun', 'helps', 'computers', 'understand', 'language', 'machine', 'learning', 'linguistics', 'powerful', 'together']\n",
      "Stems: ['nlp', 'fun', 'help', 'comput', 'understand', 'languag', 'machin', 'learn', 'linguist', 'power', 'togeth']\n",
      "Lemmas: ['nlp', 'fun', 'help', 'computer', 'understand', 'language', 'machine', 'learning', 'linguistics', 'powerful', 'together']\n",
      "Regex tokens: ['NLP', 'is', 'fun', 'It', 'helps', 'computers', 'understand', 'language', 'Machine', 'learning', 'and', 'linguistics', 'are', 'powerful', 'together']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "text = (\n",
    "    \"\"\"NLP is fun! It helps computers understand language.\n",
    "    Machine learning and linguistics are powerful together.\"\"\"\n",
    ")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print('Sentences:', sentences)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print('Words:', words)\n",
    "\n",
    "# Lowercase + remove stopwords/punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_clean = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "print('Clean words:', words_clean)\n",
    "\n",
    "# Stemming vs Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stems = [stemmer.stem(w) for w in words_clean]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words_clean]\n",
    "print('Stems:', stems)\n",
    "print('Lemmas:', lemmas)\n",
    "\n",
    "# Regex tokenizer: words with 2+ letters\n",
    "regex_tok = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
    "print('Regex tokens:', regex_tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Syntactic Analysis — POS Tagging and Chunking\n",
    "Syntactic analysis finds grammatical structure. We'll tag parts-of-speech and extract simple noun phrases (NP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('John', 'NNP'), ('bought', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('laptop', 'NN'), ('from', 'IN'), ('the', 'DT'), ('store', 'NN'), ('in', 'IN'), ('Chennai', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  bought/VBD\n",
      "  (NP a/DT new/JJ laptop/NN)\n",
      "  from/IN\n",
      "  (NP the/DT store/NN)\n",
      "  in/IN\n",
      "  (NP Chennai/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = word_tokenize(\"John bought a new laptop from the store in Chennai.\")\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print('POS tags:', pos_tags)\n",
    "\n",
    "# Simple NP chunk grammar: optional determiner, any adjectives, then a noun\n",
    "grammar = r\"NP: {<DT>?<JJ>*<NN.*>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(pos_tags)\n",
    "print(tree)  # text-based parse tree\n",
    "# Optional GUI view if supported locally:\n",
    "# tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Semantic Analysis — Word Meaning and Entities\n",
    "We'll use WordNet for synonyms/definitions, a simple similarity measure, and NLTK's basic NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for 'computer':\n",
      "- computer.n.01 | definition: a machine for performing calculations automatically\n",
      "- calculator.n.01 | definition: an expert at calculation (or at operating calculating machines)\n",
      "Similarity(dog, cat): 0.2\n",
      "(S\n",
      "  (PERSON Google/NNP)\n",
      "  hired/VBD\n",
      "  (PERSON Sundar/NNP Pichai/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'computer'\n",
    "synsets = wn.synsets(word)\n",
    "print(f'Synsets for {word!r}:')\n",
    "for s in synsets[:3]:\n",
    "    print('-', s.name(), '| definition:', s.definition())\n",
    "\n",
    "# Simple path similarity between two senses\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print('Similarity(dog, cat):', dog.path_similarity(cat))\n",
    "\n",
    "# Named Entity Recognition (basic)\n",
    "sentence = \"Google hired Sundar Pichai in California.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "ner_tree = nltk.ne_chunk(tags)\n",
    "print(ner_tree)\n",
    "# ner_tree.draw()  # optional GUI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Discourse Analysis — Across Sentences\n",
    "We look for simple discourse markers and repeated content words to see how sentences connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. Ravi bought a phone. \n",
      "2. He liked the camera; however, the battery was weak. | markers: however\n",
      "3. Therefore, he returned the phone. | markers: therefore\n",
      "Repeated content words: ['phone', 'he', 'the']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "paragraph = (\n",
    "    \"\"\"Ravi bought a phone. He liked the camera; however, the battery was weak.\n",
    "    Therefore, he returned the phone.\"\"\"\n",
    ")\n",
    "sents = sent_tokenize(paragraph)\n",
    "markers = {'however', 'therefore', 'moreover', 'meanwhile', 'furthermore', 'nevertheless'}\n",
    "\n",
    "print('Sentences:')\n",
    "for i, s in enumerate(sents, 1):\n",
    "    found = [m for m in markers if m in s.lower()]\n",
    "    print(f'{i}.', s, ('| markers: ' + ', '.join(found)) if found else '')\n",
    "\n",
    "# Very naive repetition tracker\n",
    "all_words = [w.lower() for w in word_tokenize(paragraph) if w.isalpha()]\n",
    "counts = Counter(all_words)\n",
    "print('Repeated content words:', [w for w, c in counts.items() if c > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Pragmatic Analysis — Intent (Very Simple)\n",
    "Pragmatics looks at meaning in context. We'll create a tiny rule-based intent guesser to show the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! -> intent: greeting\n",
      "How much is this? -> intent: ask_price\n",
      "Ok bye -> intent: farewell\n",
      "Can you help? -> intent: unknown\n"
     ]
    }
   ],
   "source": [
    "def guess_intent(utterance: str) -> str:\n",
    "    u = utterance.lower().strip()\n",
    "    if any(x in u for x in ['price', 'cost', 'how much']):\n",
    "        return 'intent: ask_price'\n",
    "    if any(x in u for x in ['hi', 'hello', 'hey']):\n",
    "        return 'intent: greeting'\n",
    "    if any(x in u for x in ['bye', 'goodbye', 'see you']):\n",
    "        return 'intent: farewell'\n",
    "    return 'intent: unknown'\n",
    "\n",
    "for s in ['Hello!', 'How much is this?', 'Ok bye', 'Can you help?']:\n",
    "    print(s, '->', guess_intent(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercises\n",
    "- Change the input text and observe tokenization differences.\n",
    "- Add your own stopwords (domain-specific words) and re-run.\n",
    "- Try LancasterStemmer or SnowballStemmer and compare to PorterStemmer.\n",
    "- Modify the chunk grammar (e.g., capture prepositional phrases).\n",
    "- Look up different WordNet synsets and compare similarities.\n",
    "- Extend `guess_intent()` with more rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
