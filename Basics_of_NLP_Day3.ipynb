{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP — Day 3\n",
    "\n",
    "- **Basics of NLP**: Lexical, Syntactic, Semantic, Discourse, Pragmatic analysis\n",
    "- **Simple Chatbot Components**: NLTK, scikit-learn, corpora, preprocessing, tokenization, vectorization, Bag-of-Words, Naive Bayes\n",
    "- **Translators**: Using Google Translator class (via `googletrans`)\n",
    "- **Practical**: Build a tiny translator and a rule+ML hybrid chatbot\n",
    "\n",
    "Tip: Run cells from top to bottom. If something fails, re-run the setup cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "- Installs required packages (nltk, scikit-learn, googletrans)\n",
    "- Downloads small NLTK resources\n",
    "\n",
    "If you're offline, the download steps may skip — basic parts will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment the next two lines to install:\n",
    "# !pip install -q nltk scikit-learn googletrans==4.0.0-rc1\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download essential NLTK data quietly\n",
    "packages = [\n",
    "    'punkt',                # tokenizers\n",
    "    'stopwords',            # stopwords list\n",
    "    'wordnet',              # WordNet for lemmatization/semantics\n",
    "    'omw-1.4',              # WordNet multilingual data\n",
    "    'averaged_perceptron_tagger',  # POS tagger\n",
    "    'maxent_ne_chunker',    # NER chunker\n",
    "    'words'                 # word list for NER\n",
    "]\n",
    "for p in packages:\n",
    "    try:\n",
    "        nltk.download(p, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f'Could not download {p}:', e)\n",
    "\n",
    "print('Setup complete. ✅')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lexical Analysis (Words and Sentences)\n",
    "Lexical analysis = breaking text into units (sentences, words), normalizing, and cleaning.\n",
    "\n",
    "We'll cover:\n",
    "- **Sentence tokenization**\n",
    "- **Word tokenization**\n",
    "- **Stopword removal**\n",
    "- **Stemming vs Lemmatization**\n",
    "- A simple **Regex tokenizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "text = (\n",
    "    \"NLP is fun! It helps computers understand language.\n",
    "    Machine learning + linguistics = powerful tools.\"\n",
    ")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print('Sentences:', sentences)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print('Words:', words)\n",
    "\n",
    "# Lowercasing and stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_clean = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "print('Clean words (no stopwords/punct):', words_clean)\n",
    "\n",
    "# Stemming vs Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stems = [stemmer.stem(w) for w in words_clean]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words_clean]\n",
    "print('Stems:', stems)\n",
    "print('Lemmas:', lemmas)\n",
    "\n",
    "# Regex tokenizer: only words of 2+ letters\n",
    "regex_tok = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
    "print('Regex tokens:', regex_tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Syntactic Analysis (Grammar / Structure)\n",
    "Syntactic analysis = understanding parts of speech (POS) and phrase structures.\n",
    "\n",
    "We'll do:\n",
    "- **POS tagging**\n",
    "- **Chunking** (simple noun phrase finder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = word_tokenize(\"John bought a new laptop from the store in Chennai.\")\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print('POS tags:', pos_tags)\n",
    "\n",
    "# Define a simple noun phrase (NP) chunk grammar: determiner + adjectives* + noun\n",
    "grammar = r\"NP: {<DT>?<JJ>*<NN.*>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(pos_tags)\n",
    "print(tree)  # text-based tree\n",
    "# To visualize in a window (optional in local env):\n",
    "# tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Semantic Analysis (Meaning)\n",
    "Semantic analysis = getting meaning from words/sentences.\n",
    "\n",
    "We'll do:\n",
    "- **WordNet**: synonyms / definitions\n",
    "- **Simple similarity**\n",
    "- **Named Entity Recognition (NER)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# WordNet: look up synsets for a word\n",
    "word = 'computer'\n",
    "synsets = wn.synsets(word)\n",
    "print(f'Synsets for {word!r}:')\n",
    "for s in synsets[:3]:\n",
    "    print('-', s.name(), '| definition:', s.definition())\n",
    "\n",
    "# Simple path similarity between two senses\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print('Similarity(dog, cat):', dog.path_similarity(cat))\n",
    "\n",
    "# NER: find named entities (like PERSON, ORGANIZATION)\n",
    "sentence = \"Google hired Sundar Pichai in California.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "ner_tree = nltk.ne_chunk(tags)\n",
    "print(ner_tree)\n",
    "# ner_tree.draw()  # optional GUI tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Discourse Analysis (Across Sentences)\n",
    "Discourse analysis looks at how multiple sentences connect (coherence, reference). True coreference is advanced, but we can do small checks.\n",
    "\n",
    "Here: we will find simple **discourse markers** (e.g., 'however', 'therefore') and link basic references by repeated nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "paragraph = (\n",
    "    \"Ravi bought a phone. He liked the camera; however, the battery was weak.\n",
    "    Therefore, he returned the phone.\"\n",
    ")\n",
    "sents = sent_tokenize(paragraph)\n",
    "markers = {\n",
    "    'however', 'therefore', 'moreover', 'meanwhile', 'furthermore', 'nevertheless'\n",
    "}\n",
    "\n",
    "print('Sentences:')\n",
    "for i, s in enumerate(sents, 1):\n",
    "    found = [m for m in markers if m in s.lower()]\n",
    "    print(f'{i}.', s, ('| markers: ' + ', '.join(found)) if found else '')\n",
    "\n",
    "# Very naive entity repetition tracker\n",
    "all_words = [w.lower() for w in word_tokenize(paragraph) if w.isalpha()]\n",
    "counts = Counter(all_words)\n",
    "print('Repeated content words (possible discourse links):',\n",
    "      [w for w, c in counts.items() if c > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Pragmatic Analysis (Meaning in Context / Intent)\n",
    "Pragmatics = how context changes meaning (sarcasm, politeness, intent). This is hard to code simply; modern systems use ML with context.\n",
    "\n",
    "We will: show how the same sentence can be different based on context, and use a tiny rule to guess intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_intent(utterance: str) -> str:\n",
    "    u = utterance.lower().strip()\n",
    "    if any(x in u for x in ['price', 'cost', 'how much']):\n",
    "        return 'intent: ask_price'\n",
    "    if any(x in u for x in ['hi', 'hello', 'hey']):\n",
    "        return 'intent: greeting'\n",
    "    if any(x in u for x in ['bye', 'goodbye', 'see you']):\n",
    "        return 'intent: farewell'\n",
    "    return 'intent: unknown'\n",
    "\n",
    "samples = [\n",
    "    'Hello!',\n",
    "    'How much is this phone?',\n",
    "    'Ok bye'\n",
    "]\n",
    "for s in samples:\n",
    "    print(s, '->', guess_intent(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 — Components of a Simple Chatbot\n",
    "We'll build a tiny text classifier to map user messages to intents using:\n",
    "\n",
    "- **Preprocessing & Tokenization** (NLTK)\n",
    "- **Vectorization** (CountVectorizer = Bag of Words)\n",
    "- **Classifier** (Naive Bayes)\n",
    "\n",
    "Then, we write a simple `respond()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1) Tiny dataset (toy intents)\n",
    "texts = [\n",
    "    'hi', 'hello there', 'hey', 'good morning',\n",
    "    'bye', 'goodbye', 'see you later',\n",
    "    'what is the price', 'how much does it cost', 'price of the item',\n",
    "    'what is your name', 'who are you',\n",
    "    'can you help me', 'i need help', 'please assist'\n",
    "]\n",
    "labels = [\n",
    "    'greet','greet','greet','greet',\n",
    "    'bye','bye','bye',\n",
    "    'ask_price','ask_price','ask_price',\n",
    "    'ask_name','ask_name',\n",
    "    'ask_help','ask_help','ask_help'\n",
    "]\n",
    "\n",
    "# 2) Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# 3) Pipeline: CountVectorizer + Naive Bayes\n",
    "model = Pipeline([\n",
    "    ('vect', CountVectorizer(lowercase=True, ngram_range=(1,2))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4) Evaluate\n",
    "pred = model.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# 5) Respond function\n",
    "def respond(user_text: str) -> str:\n",
    "    intent = model.predict([user_text])[0]\n",
    "    if intent == 'greet':\n",
    "        return 'Hello! How can I help you today?'\n",
    "    if intent == 'bye':\n",
    "        return 'Goodbye! Have a great day.'\n",
    "    if intent == 'ask_price':\n",
    "        return 'This item costs $499.'\n",
    "    if intent == 'ask_name':\n",
    "        return \"I'm a simple demo bot.\"\n",
    "    if intent == 'ask_help':\n",
    "        return 'Sure, tell me what you need help with.'\n",
    "    return \"I'm not sure I understood. Could you rephrase?\"\n",
    "\n",
    "for q in ['hello', 'price please', 'who are you', 'bye']:\n",
    "    print(q, '->', respond(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Bag of Words (CountVectorizer) works (quick peek)\n",
    "- It builds a vocabulary of tokens from training text.\n",
    "- Each message becomes a vector counting token occurrences.\n",
    "- Naive Bayes learns probabilities of words per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = model.named_steps['vect']\n",
    "vocab = vect.vocabulary_\n",
    "print('Vocabulary size:', len(vocab))\n",
    "# Show a few tokens\n",
    "for token, idx in list(vocab.items())[:10]:\n",
    "    print(token, '->', idx)\n",
    "\n",
    "sample_vector = vect.transform(['hello price']).toarray()[0]\n",
    "nonzero = np.where(sample_vector>0)[0]\n",
    "print('Non-zero features for \"hello price\":', nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 — Translator (Google Translator Class)\n",
    "We'll use `googletrans` (unofficial) for quick demos. It may be unstable if Google changes endpoints.\n",
    "\n",
    "If translation fails, re-run the setup cell and ensure internet is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    print('googletrans is ready.')\n",
    "except Exception as e:\n",
    "    translator = None\n",
    "    print('Could not initialize googletrans:', e)\n",
    "\n",
    "def translate_text(text: str, dest: str = 'en', src: str = 'auto'):\n",
    "    if translator is None:\n",
    "        return '[translator unavailable]'\n",
    "    try:\n",
    "        res = translator.translate(text, dest=dest, src=src)\n",
    "        return res.text\n",
    "    except Exception as e:\n",
    "        return f'[error: {e}]'\n",
    "\n",
    "examples = [\n",
    "    ('வணக்கம்', 'en'),   # Tamil -> English\n",
    "    ('Hello, how are you?', 'ta'),  # English -> Tamil\n",
    "    ('Buenos días', 'en'), # Spanish -> English\n",
    "]\n",
    "for text, dest in examples:\n",
    "    print(f'{text!r} -> ({dest})', translate_text(text, dest=dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical — Put It Together\n",
    "- A helper that translates user input to English, gets chatbot reply, then translates back to the user's language.\n",
    "- If translator is not available, it will just reply in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilingual_respond(user_text: str, user_lang: str = 'en') -> str:\n",
    "    # Step 1: translate to English (if needed)\n",
    "    text_en = translate_text(user_text, dest='en') if user_lang != 'en' else user_text\n",
    "    # Step 2: bot respond in English\n",
    "    reply_en = respond(text_en)\n",
    "    # Step 3: translate back to user language (if needed)\n",
    "    reply_user = translate_text(reply_en, dest=user_lang) if user_lang != 'en' else reply_en\n",
    "    return reply_user\n",
    "\n",
    "tests = [\n",
    "    ('hello', 'en'),\n",
    "    ('قیمت چقدر است؟', 'fa'),   # Persian: what's the price?\n",
    "    ('precio por favor', 'es'),  # Spanish\n",
    "    ('வணக்கம்', 'ta'),          # Tamil\n",
    "]\n",
    "for text, lang in tests:\n",
    "    print(f'User({lang}):', text)\n",
    "    print('Bot:', multilingual_respond(text, user_lang=lang))\n",
    "    print('-'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
